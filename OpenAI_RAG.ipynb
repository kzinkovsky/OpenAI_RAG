{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain faiss-cpu openai langchain-community langchain-openai pypdf pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "#from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import re\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*NSOpenPanel.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for file selecting, loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float_input(prompt, default):\n",
    "    \"\"\"\n",
    "    Prompts the user to enter a float value and returns it as the result.\n",
    "    Args:\n",
    "        prompt: a hint for the user with a brief explanation what to enter,\n",
    "        default: the default value that is returned if the user enters it incorrectly\n",
    "    Returns:\n",
    "        a floating point value entered by the user or a default value \n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            value = input(prompt).strip()\n",
    "            return float(value) if value else default\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a number (integer or decimal) or leave blank for default ({default}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_int_input(prompt, default):\n",
    "    \"\"\"\n",
    "    Prompts the user to enter an integer value and returns it as the result.\n",
    "    Args:\n",
    "        prompt: a hint for the user with a brief explanation what to enter,\n",
    "        default: the default value that is returned if the user enters it incorrectly\n",
    "    Returns:\n",
    "        an integer value entered by the user or a default value \n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            value = input(prompt).strip()\n",
    "            return int(value) if value else default\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a number (integer or decimal) or leave blank for default ({default}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_file():\n",
    "    \"\"\"\n",
    "    Allows the user to select a PDF file using the tkinter library's UI and returns the path to the file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hide the main Tkinter window\n",
    "    Tk().withdraw()\n",
    "    # Open file selection dialog\n",
    "    file_path = askopenfilename(title=\"Select a PDF file\", filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    if file_path:\n",
    "        print(f\"Selected file: {file_path}\")\n",
    "    else:\n",
    "        print(\"No file selected\")\n",
    "        file_path = None\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    \"\"\"\n",
    "    Reads a PDF file from the specified path using langchain PyPDFLoader.\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "    Returns:\n",
    "        document: list containing [Document(metadata={'source': '...', 'page': ...}, page_content='...text...', ...] \n",
    "    \"\"\"\n",
    "    # Load PDF document\n",
    "    loader = PyPDFLoader(path)\n",
    "    document = loader.load()\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(texts): \n",
    "    \"\"\"\n",
    "    Cleans text\n",
    "    Args:\n",
    "        texts: text after splitting into chanks using langchain RecursiveCharacterTextSplitter.\n",
    "    Returns:\n",
    "        texts: cleaned text \n",
    "    \"\"\"\n",
    "    \n",
    "    for text in texts:\n",
    "        # Remove page numbers with only of numbers (max 4 digits) with optional spaces, hyphens or # around\n",
    "        text.page_content = re.sub(r'^[\\s#-]*\\d{1,4}[\\s#-]*$', '', text.page_content)\n",
    "        # Remove suffixes like 'st', 'nd', 'rd', 'th', 'т', 'й' (up to two letters after the number)\n",
    "        text.page_content = re.sub(r'(\\d+)-?([a-zA-Zа-яА-Я]{1,2})', r'\\1', text.page_content)\n",
    "        # Remove # and № in front of numbers\n",
    "        text.page_content = re.sub(r'[#№](\\d+)', r'\\1', text.page_content)\n",
    "         # Remove newline characters\n",
    "        text.page_content = text.page_content.replace('\\n', ' ')\n",
    "\n",
    "    # Remove empty pages\n",
    "    texts = [text for text in texts if text.page_content]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for configuring and creating a FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAISS_config(query_to_tune):\n",
    "    \"\"\"\n",
    "    Prompts the user to enter values of some FAISS parameters to tune it.\n",
    "    Args:\n",
    "        query_to_tune: a passed request from the user to change or use default parameters ('tune' / any other input)\n",
    "    Function called inside:\n",
    "        safe_number_input(\"Hint\", value_by_default)\n",
    "    Returns:\n",
    "       config: dictionary with FAISS parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    if query_to_tune == 'tune':\n",
    "        config = {\n",
    "            \"chunk_size\": safe_int_input(\"Input chunk size to split text for RAG (by default 2300): \", 2300),\n",
    "            \"chunk_overlap\": safe_int_input(\"Input chunk overlap (by default 400): \", 400),\n",
    "            \"n_chunks_in_context\": safe_int_input(\"Input number of top chunks retrieved from RAG vector store (by default 2): \", 2)\n",
    "        }\n",
    "    else:\n",
    "        config = {\n",
    "            \"chunk_size\": 2300,\n",
    "            \"chunk_overlap\": 400,\n",
    "            \"n_chunks_in_context\": 2\n",
    "        }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splite_encode_document(document, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Splits and encodes a document into a vector store using OpenAI embeddings and FAISS.\n",
    "    Args:\n",
    "        document: a pdf file read using langchain PyPDFLoader.\n",
    "        chunk_size: text chunk size.\n",
    "        chunk_overlap: overlap between chunks.\n",
    "    Function called inside:\n",
    "        clean_text(texts)\n",
    "    Returns:\n",
    "        vectorstore: a FAISS vector store containing the encoded texts.\n",
    "    \"\"\"\n",
    "    # Split document into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(document)\n",
    "    texts = clean_text(texts)\n",
    "    \n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve context, get answer to a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, query_retriever):\n",
    "    \"\"\"\n",
    "    Retrieves a relevant context.\n",
    "    Args:\n",
    "        question: str with user question.\n",
    "        query_retriever: FAISS query_retriever.\n",
    "    Returns:\n",
    "        context: list with str of context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve relevant chanks for the given question\n",
    "    retrieved_chanks = query_retriever.invoke(question)\n",
    "\n",
    "    # Concatenate retrieved chanks in one context\n",
    "    context = [chank.page_content for chank in retrieved_chanks]\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    \"\"\"A class for formatting the output data of a model according to a given structure.\"\"\"\n",
    "    answer: str = Field(description=\"Generates an answer to a query based on a given context.\")\n",
    "    context: list[str] = Field(description=\"The context used to generate the answer.\")\n",
    "    question: str = Field(description=\"The question that was answered.\")\n",
    "\n",
    "\n",
    "def create_question_answer_from_context_chain(llm):\n",
    "    \"\"\"\n",
    "    Creates a langchain question-answer chain using a given language model.\n",
    "    Args:\n",
    "        llm: LLM model used for getting answers.\n",
    "    Returns:\n",
    "        question_answer_chain: langchain object - chain for getting GenAI's answer to the user's question based on context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create template for queries\n",
    "    question_answer_prompt_template = \"\"\" \n",
    "    For the question below, provide a concise but suffice answer based ONLY on the provided context:\n",
    "    {context}\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PromptTemplate object with the specified template and input variables\n",
    "    question_answer_from_context_prompt = PromptTemplate(\n",
    "        template=question_answer_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Create the chain, which processes the prompt template with the llm\n",
    "    question_answer_chain = question_answer_from_context_prompt | llm\n",
    "    \n",
    "    return question_answer_chain\n",
    "\n",
    "\n",
    "def answer_question_from_context(question, context, question_answer_chain):\n",
    "    \"\"\"\n",
    "    Answers to user's question using the given context.\n",
    "    Args:\n",
    "        question: user's question\n",
    "        context: retrieved context for user's question\n",
    "        question_answer_chain: processing chain obtained from create_question_answer_from_context_chai function.\n",
    "    Returns:\n",
    "        structured_result: the model output structured as QuestionAnswerFromContext object\n",
    "        output: the model row output\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "    print(\"Preparing results ...\")\n",
    "\n",
    "    output = question_answer_chain.invoke(input_data) \n",
    "    \n",
    "    raw_result = {\n",
    "    'answer': output.content,\n",
    "    'context': input_data['context'],\n",
    "    'question': input_data['question']\n",
    "    }\n",
    "    try:\n",
    "        structured_result = QuestionAnswerFromContext(**raw_result)  \n",
    "    except ValidationError as e:\n",
    "        print(\"Validation Error:\", e)\n",
    "        structured_result = None\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return structured_result, output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main script to run the AI assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\n=== AI Assistant for Answering Questions About Texts ===\\n=== OpenAI with Retrieval-Augmented Generation (RAG) ===\\n\\n\")\n",
    "\n",
    "    # Request path to PDF file with text, load and read it\n",
    "    user_query1 = \"\"\n",
    "    while True:\n",
    "        print(\"Please select a pdf file in the window that appears\")\n",
    "        file_path = select_file()\n",
    "        if file_path:\n",
    "            print(f\"Selected PDF file loading and reading...\")\n",
    "            try:\n",
    "                document = read_pdf(file_path)\n",
    "                print(\"Document uploaded successfully!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file: {e}\")\n",
    "        else:\n",
    "            print(\"File not selected or not found. Please, make again a choice in the window that appears.\")\n",
    "            user_query1 = input(\"To try again type any symbol, to exit type exit: \").strip().lower()\n",
    "            if user_query1 in {\"exit\"}:\n",
    "                break\n",
    "\n",
    "    if user_query1 in {\"exit\"}:\n",
    "        return \"The program has been successfully completed. Bye!\"\n",
    "    \n",
    "    # Request for RAG tuning parameters\n",
    "    user_query2 = input(\"To tune RAG type 'tune'. To use tuning by defolt type any symbol: \").strip().lower()\n",
    "    RAG_config = FAISS_config(user_query2)\n",
    "    chunk_size = RAG_config['chunk_size']\n",
    "    chunk_overlap = RAG_config['chunk_overlap']\n",
    "    n_chunks_in_context = RAG_config['n_chunks_in_context']\n",
    "    \n",
    "    print(\"Preparing a retriever with your text...\")\n",
    "    # split document and create FAISS vectorestore \n",
    "    vectorestore = splite_encode_document(document, chunk_size, chunk_overlap)\n",
    "\n",
    "    # Create retriever\n",
    "    query_retriever = vectorestore.as_retriever(search_kwargs={\"k\": n_chunks_in_context})\n",
    "    print(\"The retriever is ready to work\")\n",
    "\n",
    "    # Basic workflow: user request – AI response\n",
    "    while True:\n",
    "        print(\"\\nYou can ask the AI assistant in the window that appears. To exit the program, type 'exit'.\\n\")\n",
    "        user_query_toAI = input(\"Your question: \").strip()\n",
    "        \n",
    "        if user_query_toAI.lower() in {\"exit\"}:\n",
    "            return \"Thank you for using the AI assistant. Bye!\"\n",
    "        \n",
    "        # LLM object with parameters by default\n",
    "        llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4o\", max_tokens=2000)\n",
    "        \n",
    "        # Code for the hidden ability to change LLM parameters. \n",
    "        if user_query_toAI.lower() in {\"tune llm\"}:\n",
    "            print(\"You just requested 'tune llm' for llm tuning\\n\")\n",
    "            temperature = safe_float_input(\"temperature (0.7 by default): \", 0.7)\n",
    "            model_name = input(\"model name (press enter for gpt-4o by default): \")\n",
    "            if not model_name: \n",
    "                model_name = \"gpt-4o\"\n",
    "            max_tokens = safe_int_input(\"max tokens (2000 by default): \", 2000)\n",
    "            llm = ChatOpenAI(temperature=temperature, model_name=model_name, max_tokens=max_tokens)\n",
    "            user_query_toAI = input(\"LLM tuning done\\nYour question to the AI assistant: \").strip()\n",
    "\n",
    "        try:\n",
    "            question_answer_chain = create_question_answer_from_context_chain(llm)\n",
    "            context = retrieve_context(user_query_toAI, query_retriever)\n",
    "\n",
    "            structured_result, raw_result = answer_question_from_context(user_query_toAI, context, question_answer_chain)\n",
    "            print(f\"{'Your question:':<15}{user_query_toAI}\")\n",
    "            print(f\"{'AI answer:':<15}{structured_result.answer}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing request: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
